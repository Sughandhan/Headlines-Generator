{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headlines Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing a text predictor model and using it to generate headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will prepare the sequence data to be used in a LSTM (special type of RNN)\n",
    "* We will build and train a model to perform word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = 'Articles/'\n",
    "\n",
    "all_headlines = []\n",
    "\n",
    "for fn in os.listdir(articles_dir):\n",
    "    if 'Articles' in fn:\n",
    "        df = pd.read_csv(articles_dir + fn) #reading  all the headlines from our csv file\n",
    "        all_headlines.extend(list(df.headline.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9335\n"
     ]
    }
   ],
   "source": [
    "print(len(all_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Finding an Expansive View  of a Forgotten People in Niger',\n",
       " 'And Now,  the Dreaded Trump Curse',\n",
       " 'Venezuela’s Descent Into Dictatorship',\n",
       " 'Stain Permeates Basketball Blue Blood',\n",
       " 'Taking Things for Granted',\n",
       " 'The Caged Beast Awakens',\n",
       " 'An Ever-Unfolding Story',\n",
       " 'O’Reilly Thrives as Settlements Add Up',\n",
       " 'Mouse Infestation',\n",
       " 'Divide in G.O.P. Now Threatens Trump Tax Plan',\n",
       " 'Variety Puzzle: Acrostic',\n",
       " 'They Can Hit a Ball 400 Feet. But Play Catch? That’s Tricky.',\n",
       " 'In Trump Country, Shock at Trump Budget Cuts',\n",
       " 'Why Is This Hate Different From All Other Hate?',\n",
       " 'Pick Your Favorite Ethical Offender',\n",
       " 'My Son’s Growing Black Pride',\n",
       " 'Jerks and the Start-Ups They Ruin',\n",
       " 'Trump  Needs  a Brain',\n",
       " 'Manhood in the Age of Trump',\n",
       " 'The Value of a Black College']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732\n"
     ]
    }
   ],
   "source": [
    "#Counting the headlines labled as \"Unknown\"\n",
    "unknown_count = 0;\n",
    "for line in all_headlines:\n",
    "    if line=='Unknown':\n",
    "        unknown_count += 1\n",
    "print(unknown_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the \"Unknown\" headlines\n",
    "all_headlines = [ line for line in all_headlines if line != \"Unknown\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8603\n"
     ]
    }
   ],
   "source": [
    "print(len(all_headlines))  #9335 - 732 = 8603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Finding an Expansive View  of a Forgotten People in Niger',\n",
       " 'And Now,  the Dreaded Trump Curse',\n",
       " 'Venezuela’s Descent Into Dictatorship',\n",
       " 'Stain Permeates Basketball Blue Blood',\n",
       " 'Taking Things for Granted',\n",
       " 'The Caged Beast Awakens',\n",
       " 'An Ever-Unfolding Story',\n",
       " 'O’Reilly Thrives as Settlements Add Up',\n",
       " 'Mouse Infestation',\n",
       " 'Divide in G.O.P. Now Threatens Trump Tax Plan',\n",
       " 'Variety Puzzle: Acrostic',\n",
       " 'They Can Hit a Ball 400 Feet. But Play Catch? That’s Tricky.',\n",
       " 'In Trump Country, Shock at Trump Budget Cuts',\n",
       " 'Why Is This Hate Different From All Other Hate?',\n",
       " 'Pick Your Favorite Ethical Offender',\n",
       " 'My Son’s Growing Black Pride',\n",
       " 'Jerks and the Start-Ups They Ruin',\n",
       " 'Trump  Needs  a Brain',\n",
       " 'Manhood in the Age of Trump',\n",
       " 'The Value of a Black College']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising our words in all the headlines\n",
    "#the Tokenizer class will take care of removing the punctuations and converting the words to lowercase\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_num_of_words = len(tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:  11753\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words: \", total_num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'on': 8,\n",
       " 'is': 9,\n",
       " 'trump': 10,\n",
       " 'with': 11,\n",
       " 'new': 12,\n",
       " 'at': 13,\n",
       " 'how': 14,\n",
       " 'what': 15,\n",
       " 'you': 16,\n",
       " 'an': 17,\n",
       " 'from': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'trump’s': 21,\n",
       " 'your': 22,\n",
       " 'are': 23,\n",
       " 'not': 24,\n",
       " 'be': 25,\n",
       " 'season': 26,\n",
       " 's': 27,\n",
       " 'u': 28,\n",
       " 'that': 29,\n",
       " 'i': 30,\n",
       " 'by': 31,\n",
       " 'about': 32,\n",
       " 'but': 33,\n",
       " 'episode': 34,\n",
       " 'can': 35,\n",
       " 'do': 36,\n",
       " 'up': 37,\n",
       " 'when': 38,\n",
       " 'york': 39,\n",
       " 'over': 40,\n",
       " 'this': 41,\n",
       " 'out': 42,\n",
       " 'no': 43,\n",
       " '’': 44,\n",
       " 'why': 45,\n",
       " 'more': 46,\n",
       " 'p': 47,\n",
       " '‘the': 48,\n",
       " 'after': 49,\n",
       " 'o': 50,\n",
       " 'will': 51,\n",
       " 'my': 52,\n",
       " 'may': 53,\n",
       " 'it’s': 54,\n",
       " 'or': 55,\n",
       " 'health': 56,\n",
       " 'war': 57,\n",
       " 'who': 58,\n",
       " 'his': 59,\n",
       " 'we': 60,\n",
       " 'its': 61,\n",
       " 'teaching': 62,\n",
       " 'questions': 63,\n",
       " 'g': 64,\n",
       " 'president': 65,\n",
       " 'was': 66,\n",
       " 'house': 67,\n",
       " 'one': 68,\n",
       " 'have': 69,\n",
       " '1': 70,\n",
       " 'should': 71,\n",
       " 'get': 72,\n",
       " 'today': 73,\n",
       " 'into': 74,\n",
       " 'all': 75,\n",
       " 'now': 76,\n",
       " '2': 77,\n",
       " 'life': 78,\n",
       " 'home': 79,\n",
       " 'our': 80,\n",
       " 'don’t': 81,\n",
       " 'plan': 82,\n",
       " 'good': 83,\n",
       " 'has': 84,\n",
       " '3': 85,\n",
       " 'like': 86,\n",
       " 'says': 87,\n",
       " 'white': 88,\n",
       " 'first': 89,\n",
       " 'he': 90,\n",
       " 'trade': 91,\n",
       " 'world': 92,\n",
       " 'back': 93,\n",
       " 'love': 94,\n",
       " 'women': 95,\n",
       " 'too': 96,\n",
       " 'mr': 97,\n",
       " 'so': 98,\n",
       " 'big': 99,\n",
       " 'russia': 100,\n",
       " 'right': 101,\n",
       " 'their': 102,\n",
       " 'go': 103,\n",
       " 'recap': 104,\n",
       " 'off': 105,\n",
       " 'times': 106,\n",
       " 'her': 107,\n",
       " 'going': 108,\n",
       " 'north': 109,\n",
       " 'china': 110,\n",
       " 'if': 111,\n",
       " 'donald': 112,\n",
       " 'time': 113,\n",
       " 'they': 114,\n",
       " 'just': 115,\n",
       " 'year': 116,\n",
       " 'care': 117,\n",
       " 'still': 118,\n",
       " 'help': 119,\n",
       " 'way': 120,\n",
       " 'day': 121,\n",
       " 'could': 122,\n",
       " 'say': 123,\n",
       " 'where': 124,\n",
       " 'black': 125,\n",
       " '6': 126,\n",
       " 'what’s': 127,\n",
       " 'city': 128,\n",
       " 'f': 129,\n",
       " 'end': 130,\n",
       " 'america': 131,\n",
       " 'variety': 132,\n",
       " '5': 133,\n",
       " 'power': 134,\n",
       " 'better': 135,\n",
       " 'activities': 136,\n",
       " 'korea': 137,\n",
       " 'man': 138,\n",
       " 'race': 139,\n",
       " 'make': 140,\n",
       " 'news': 141,\n",
       " 'old': 142,\n",
       " 'two': 143,\n",
       " 'tax': 144,\n",
       " 'than': 145,\n",
       " 'some': 146,\n",
       " 'rules': 147,\n",
       " 'state': 148,\n",
       " 'heart': 149,\n",
       " 'b': 150,\n",
       " 'people': 151,\n",
       " 'own': 152,\n",
       " 'american': 153,\n",
       " 'c': 154,\n",
       " 'democrats': 155,\n",
       " 'gun': 156,\n",
       " 'political': 157,\n",
       " 'court': 158,\n",
       " 'syria': 159,\n",
       " 'bill': 160,\n",
       " 'work': 161,\n",
       " 'vs': 162,\n",
       " 'family': 163,\n",
       " 'great': 164,\n",
       " 'real': 165,\n",
       " 'next': 166,\n",
       " 'bad': 167,\n",
       " 'change': 168,\n",
       " 'little': 169,\n",
       " 'does': 170,\n",
       " 'another': 171,\n",
       " 'me': 172,\n",
       " 'against': 173,\n",
       " 'again': 174,\n",
       " 'chief': 175,\n",
       " 'food': 176,\n",
       " 'school': 177,\n",
       " 'past': 178,\n",
       " '2017': 179,\n",
       " 'take': 180,\n",
       " 'obama': 181,\n",
       " 'would': 182,\n",
       " 'want': 183,\n",
       " 'law': 184,\n",
       " 'deal': 185,\n",
       " 'long': 186,\n",
       " 'takes': 187,\n",
       " 'republicans': 188,\n",
       " 'death': 189,\n",
       " 'high': 190,\n",
       " 'e': 191,\n",
       " 'case': 192,\n",
       " '7': 193,\n",
       " 'n': 194,\n",
       " 'years': 195,\n",
       " 'then': 196,\n",
       " 'save': 197,\n",
       " 'talk': 198,\n",
       " 'fight': 199,\n",
       " 'mind': 200,\n",
       " 'money': 201,\n",
       " 'call': 202,\n",
       " 'picture': 203,\n",
       " 'student': 204,\n",
       " '—': 205,\n",
       " 'crisis': 206,\n",
       " 'need': 207,\n",
       " 'vote': 208,\n",
       " 'history': 209,\n",
       " 'policy': 210,\n",
       " 'down': 211,\n",
       " 'behind': 212,\n",
       " 'children': 213,\n",
       " 'story': 214,\n",
       " 'police': 215,\n",
       " 'pay': 216,\n",
       " 'can’t': 217,\n",
       " 'ex': 218,\n",
       " 'party': 219,\n",
       " 'men': 220,\n",
       " 'under': 221,\n",
       " 'place': 222,\n",
       " 'climate': 223,\n",
       " 'battle': 224,\n",
       " 'facebook': 225,\n",
       " 'politics': 226,\n",
       " 'future': 227,\n",
       " 'risk': 228,\n",
       " 'justice': 229,\n",
       " 'wall': 230,\n",
       " 'us': 231,\n",
       " 'homes': 232,\n",
       " 'see': 233,\n",
       " 'really': 234,\n",
       " 'sex': 235,\n",
       " 'being': 236,\n",
       " 'before': 237,\n",
       " 'think': 238,\n",
       " 'won’t': 239,\n",
       " 'tariffs': 240,\n",
       " 'college': 241,\n",
       " 'art': 242,\n",
       " 'win': 243,\n",
       " 'here': 244,\n",
       " 'rise': 245,\n",
       " 'keep': 246,\n",
       " 'eat': 247,\n",
       " 'march': 248,\n",
       " 'immigration': 249,\n",
       " 'republican': 250,\n",
       " 'open': 251,\n",
       " 'south': 252,\n",
       " 'start': 253,\n",
       " 'l': 254,\n",
       " '2018': 255,\n",
       " 'attack': 256,\n",
       " 'left': 257,\n",
       " 'free': 258,\n",
       " '4': 259,\n",
       " 'young': 260,\n",
       " 'much': 261,\n",
       " 'making': 262,\n",
       " 'child': 263,\n",
       " 'without': 264,\n",
       " 'california': 265,\n",
       " 'states': 266,\n",
       " 'russian': 267,\n",
       " 'fear': 268,\n",
       " 'live': 269,\n",
       " 'dead': 270,\n",
       " 'leader': 271,\n",
       " 'anti': 272,\n",
       " 'secret': 273,\n",
       " 'age': 274,\n",
       " 'there': 275,\n",
       " 'security': 276,\n",
       " 'best': 277,\n",
       " 'students': 278,\n",
       " 'kids': 279,\n",
       " 'obamacare': 280,\n",
       " 'fix': 281,\n",
       " 'border': 282,\n",
       " 'ban': 283,\n",
       " 'taking': 284,\n",
       " 'other': 285,\n",
       " 'book': 286,\n",
       " 'top': 287,\n",
       " 'isn’t': 288,\n",
       " 'them': 289,\n",
       " '8': 290,\n",
       " 'test': 291,\n",
       " 'french': 292,\n",
       " 'know': 293,\n",
       " 'judge': 294,\n",
       " 'night': 295,\n",
       " 'comey': 296,\n",
       " 'subway': 297,\n",
       " '000': 298,\n",
       " 'office': 299,\n",
       " 'last': 300,\n",
       " 'nuclear': 301,\n",
       " 'control': 302,\n",
       " 'living': 303,\n",
       " 'most': 304,\n",
       " 'only': 305,\n",
       " 'parents': 306,\n",
       " 'words': 307,\n",
       " 'did': 308,\n",
       " '10': 309,\n",
       " 'close': 310,\n",
       " 'game': 311,\n",
       " 'cancer': 312,\n",
       " 'less': 313,\n",
       " 'vietnam': 314,\n",
       " 'find': 315,\n",
       " 'inquiry': 316,\n",
       " 'face': 317,\n",
       " 'friday': 318,\n",
       " 'election': 319,\n",
       " 'stand': 320,\n",
       " 'even': 321,\n",
       " 'social': 322,\n",
       " 'star': 323,\n",
       " 'while': 324,\n",
       " 'him': 325,\n",
       " 'let': 326,\n",
       " 'lead': 327,\n",
       " 'fire': 328,\n",
       " 'brooklyn': 329,\n",
       " 'play': 330,\n",
       " 'act': 331,\n",
       " 'k': 332,\n",
       " 'mindful': 333,\n",
       " 'dies': 334,\n",
       " 'senate': 335,\n",
       " 'president’s': 336,\n",
       " 'many': 337,\n",
       " 'second': 338,\n",
       " 'lost': 339,\n",
       " 'gets': 340,\n",
       " 'debate': 341,\n",
       " 'use': 342,\n",
       " 'problem': 343,\n",
       " 'metoo': 344,\n",
       " 'schools': 345,\n",
       " 'acrostic': 346,\n",
       " 'budget': 347,\n",
       " 'three': 348,\n",
       " 'ask': 349,\n",
       " 'million': 350,\n",
       " 'far': 351,\n",
       " 'faces': 352,\n",
       " 'said': 353,\n",
       " 'truth': 354,\n",
       " 'side': 355,\n",
       " 'congress': 356,\n",
       " 'washington': 357,\n",
       " 'mueller': 358,\n",
       " 'media': 359,\n",
       " 'puzzle': 360,\n",
       " 'country': 361,\n",
       " 'lesson': 362,\n",
       " 'door': 363,\n",
       " 'getting': 364,\n",
       " 'stop': 365,\n",
       " 'wants': 366,\n",
       " 'hard': 367,\n",
       " 'travel': 368,\n",
       " 'presidency': 369,\n",
       " 'chinese': 370,\n",
       " 'these': 371,\n",
       " 'talks': 372,\n",
       " 'market': 373,\n",
       " 'era': 374,\n",
       " 'hope': 375,\n",
       " 'britain': 376,\n",
       " 'missing': 377,\n",
       " 'look': 378,\n",
       " 'military': 379,\n",
       " 'things': 380,\n",
       " 'cuts': 381,\n",
       " 'favorite': 382,\n",
       " 'king': 383,\n",
       " 'supreme': 384,\n",
       " 'rights': 385,\n",
       " 'role': 386,\n",
       " 'job': 387,\n",
       " '…': 388,\n",
       " 'mailbag': 389,\n",
       " 'street': 390,\n",
       " 'risks': 391,\n",
       " 'looking': 392,\n",
       " 'safety': 393,\n",
       " 'building': 394,\n",
       " 'come': 395,\n",
       " 'plans': 396,\n",
       " 'data': 397,\n",
       " 'democracy': 398,\n",
       " 'tell': 399,\n",
       " '‘a': 400,\n",
       " 'global': 401,\n",
       " 'ready': 402,\n",
       " 'finding': 403,\n",
       " 'needs': 404,\n",
       " 'woman': 405,\n",
       " 'move': 406,\n",
       " 'yes': 407,\n",
       " 'teenagers': 408,\n",
       " 'across': 409,\n",
       " 'near': 410,\n",
       " 'break': 411,\n",
       " 'putin': 412,\n",
       " 'red': 413,\n",
       " 'were': 414,\n",
       " 'made': 415,\n",
       " 'wrong': 416,\n",
       " 'turn': 417,\n",
       " 'de': 418,\n",
       " 'former': 419,\n",
       " 'west': 420,\n",
       " 'cut': 421,\n",
       " 'trial': 422,\n",
       " 'walking': 423,\n",
       " 'hate': 424,\n",
       " 'dept': 425,\n",
       " 'fast': 426,\n",
       " 'you’re': 427,\n",
       " 'americans’': 428,\n",
       " 'found': 429,\n",
       " 'days': 430,\n",
       " 'feel': 431,\n",
       " 'try': 432,\n",
       " 'learning': 433,\n",
       " 'teachers': 434,\n",
       " 'air': 435,\n",
       " 'fake': 436,\n",
       " 'voters': 437,\n",
       " 'drug': 438,\n",
       " 'jobs': 439,\n",
       " 'early': 440,\n",
       " 'abuse': 441,\n",
       " 'france': 442,\n",
       " 'cuomo': 443,\n",
       " '’s': 444,\n",
       " 'shutdown': 445,\n",
       " 'science': 446,\n",
       " 'must': 447,\n",
       " 'action': 448,\n",
       " 'winter': 449,\n",
       " 'pick': 450,\n",
       " 'dream': 451,\n",
       " 'education': 452,\n",
       " 'friends': 453,\n",
       " 'team': 454,\n",
       " 'bannon': 455,\n",
       " 'aid': 456,\n",
       " 'coming': 457,\n",
       " 'put': 458,\n",
       " '‘i': 459,\n",
       " 'crossword': 460,\n",
       " 'united': 461,\n",
       " 'doctor': 462,\n",
       " 'isis': 463,\n",
       " 'speech': 464,\n",
       " 'john': 465,\n",
       " 'americans': 466,\n",
       " 'shows': 467,\n",
       " 'v': 468,\n",
       " 'pain': 469,\n",
       " 'sessions': 470,\n",
       " 'support': 471,\n",
       " 'path': 472,\n",
       " 'got': 473,\n",
       " 'goes': 474,\n",
       " 'week': 475,\n",
       " 'class': 476,\n",
       " 'set': 477,\n",
       " 'voice': 478,\n",
       " 'makes': 479,\n",
       " 'body': 480,\n",
       " 'challenge': 481,\n",
       " 'give': 482,\n",
       " 'sale': 483,\n",
       " 'tv': 484,\n",
       " 'show': 485,\n",
       " 'blood': 486,\n",
       " 'that’s': 487,\n",
       " 'hot': 488,\n",
       " '‘billions’': 489,\n",
       " 'learn': 490,\n",
       " 'search': 491,\n",
       " 'run': 492,\n",
       " 'national': 493,\n",
       " 'well': 494,\n",
       " 'memory': 495,\n",
       " 'low': 496,\n",
       " 'meeting': 497,\n",
       " 'small': 498,\n",
       " 'readers': 499,\n",
       " 'fighting': 500,\n",
       " 'silence': 501,\n",
       " 'sports': 502,\n",
       " 'self': 503,\n",
       " 'leaders': 504,\n",
       " 'baby': 505,\n",
       " 'room': 506,\n",
       " 'loss': 507,\n",
       " 'america’s': 508,\n",
       " 'reality': 509,\n",
       " 'business': 510,\n",
       " 'rising': 511,\n",
       " 'tells': 512,\n",
       " 'idea': 513,\n",
       " 'meet': 514,\n",
       " 'd': 515,\n",
       " 'israel': 516,\n",
       " 'doesn’t': 517,\n",
       " 'threat': 518,\n",
       " 'slow': 519,\n",
       " 'claims': 520,\n",
       " 'away': 521,\n",
       " 'been': 522,\n",
       " 'warm': 523,\n",
       " 'trip': 524,\n",
       " 'price': 525,\n",
       " 'town': 526,\n",
       " 'becomes': 527,\n",
       " 'stephen': 528,\n",
       " 'legal': 529,\n",
       " 'through': 530,\n",
       " 'looks': 531,\n",
       " 'florida': 532,\n",
       " 'modern': 533,\n",
       " 'james': 534,\n",
       " 'pressure': 535,\n",
       " 'had': 536,\n",
       " 'tech': 537,\n",
       " 'might': 538,\n",
       " 'i’m': 539,\n",
       " 't': 540,\n",
       " 'question': 541,\n",
       " 'comes': 542,\n",
       " 'view': 543,\n",
       " 'brain': 544,\n",
       " 'others': 545,\n",
       " 'ways': 546,\n",
       " 'raise': 547,\n",
       " 'wins': 548,\n",
       " 'list': 549,\n",
       " 'fashion': 550,\n",
       " 'syrian': 551,\n",
       " 'island': 552,\n",
       " 'music': 553,\n",
       " 'economy': 554,\n",
       " 'road': 555,\n",
       " 'report': 556,\n",
       " 'moment': 557,\n",
       " 'jimmy': 558,\n",
       " 'video': 559,\n",
       " 'force': 560,\n",
       " 'tests': 561,\n",
       " 'tied': 562,\n",
       " 'fish': 563,\n",
       " 'didn’t': 564,\n",
       " 'post': 565,\n",
       " 'met': 566,\n",
       " 'officials': 567,\n",
       " 'r': 568,\n",
       " 'matter': 569,\n",
       " 'lives': 570,\n",
       " 'mother': 571,\n",
       " 'he’s': 572,\n",
       " 'guns': 573,\n",
       " 'here’s': 574,\n",
       " 'allies': 575,\n",
       " 'leave': 576,\n",
       " 'she': 577,\n",
       " 'aide': 578,\n",
       " 'meets': 579,\n",
       " 'crash': 580,\n",
       " 'dead’': 581,\n",
       " 'fall': 582,\n",
       " 'ever': 583,\n",
       " 'bar': 584,\n",
       " 'losing': 585,\n",
       " 'governor': 586,\n",
       " 'dr': 587,\n",
       " 'inside': 588,\n",
       " 'turns': 589,\n",
       " 'economic': 590,\n",
       " 'spring': 591,\n",
       " 'scandal': 592,\n",
       " '9': 593,\n",
       " 'word': 594,\n",
       " 'broken': 595,\n",
       " 'lose': 596,\n",
       " 'beat': 597,\n",
       " 'easy': 598,\n",
       " '100': 599,\n",
       " 'target': 600,\n",
       " 'match': 601,\n",
       " 'attacks': 602,\n",
       " 'safe': 603,\n",
       " 'workers': 604,\n",
       " 'choice': 605,\n",
       " 'girls': 606,\n",
       " 'texas': 607,\n",
       " 'cold': 608,\n",
       " 'push': 609,\n",
       " 'key': 610,\n",
       " 'm': 611,\n",
       " 'europe': 612,\n",
       " 'never': 613,\n",
       " 'once': 614,\n",
       " 'oil': 615,\n",
       " 'pregnancy': 616,\n",
       " 'privacy': 617,\n",
       " 'advice': 618,\n",
       " 'co': 619,\n",
       " 'weight': 620,\n",
       " 'paid': 621,\n",
       " 'dear': 622,\n",
       " 'success': 623,\n",
       " 'worth': 624,\n",
       " 'olympics': 625,\n",
       " 'blame': 626,\n",
       " 'limits': 627,\n",
       " 'fears': 628,\n",
       " 'center': 629,\n",
       " 'democratic': 630,\n",
       " 'contest': 631,\n",
       " 'women’s': 632,\n",
       " 'tale': 633,\n",
       " 'simple': 634,\n",
       " 'common': 635,\n",
       " 'growth': 636,\n",
       " 'longer': 637,\n",
       " 'russians': 638,\n",
       " 'any': 639,\n",
       " 'finale': 640,\n",
       " 'immigrants': 641,\n",
       " 'who’s': 642,\n",
       " 'abortion': 643,\n",
       " 'leads': 644,\n",
       " 'killed': 645,\n",
       " 'both': 646,\n",
       " 'car': 647,\n",
       " 'spending': 648,\n",
       " 'edge': 649,\n",
       " 'between': 650,\n",
       " 'light': 651,\n",
       " 'cities': 652,\n",
       " 'part': 653,\n",
       " 'energy': 654,\n",
       " 'something': 655,\n",
       " 'bring': 656,\n",
       " 'repeal': 657,\n",
       " 'fans': 658,\n",
       " 'grows': 659,\n",
       " 'freedom': 660,\n",
       " 'line': 661,\n",
       " 'quiet': 662,\n",
       " 'four': 663,\n",
       " 'nation': 664,\n",
       " 'human': 665,\n",
       " 'beyond': 666,\n",
       " 'ethics': 667,\n",
       " 'space': 668,\n",
       " 'ryan': 669,\n",
       " 'wish': 670,\n",
       " 'hopes': 671,\n",
       " 'spy': 672,\n",
       " 'raising': 673,\n",
       " '15': 674,\n",
       " 'labor': 675,\n",
       " 'manhattan': 676,\n",
       " 'hollywood': 677,\n",
       " 'shooting': 678,\n",
       " 'defense': 679,\n",
       " 'fed': 680,\n",
       " 'review': 681,\n",
       " 'happy': 682,\n",
       " 'hit': 683,\n",
       " 'different': 684,\n",
       " 'decision': 685,\n",
       " 'older': 686,\n",
       " 'birth': 687,\n",
       " 'sets': 688,\n",
       " 'lessons': 689,\n",
       " 'fine': 690,\n",
       " 'gay': 691,\n",
       " 'public': 692,\n",
       " 'train': 693,\n",
       " 'jail': 694,\n",
       " 'dog': 695,\n",
       " 'speak': 696,\n",
       " 'familiar': 697,\n",
       " 'service': 698,\n",
       " 'premiere': 699,\n",
       " 'york’s': 700,\n",
       " 'few': 701,\n",
       " 'walk': 702,\n",
       " 'record': 703,\n",
       " 'valley': 704,\n",
       " 'fresh': 705,\n",
       " 'puts': 706,\n",
       " 'every': 707,\n",
       " 'welcome': 708,\n",
       " 'head': 709,\n",
       " 'stay': 710,\n",
       " 'mean': 711,\n",
       " 'water': 712,\n",
       " 'offer': 713,\n",
       " 'china’s': 714,\n",
       " 'embrace': 715,\n",
       " 'become': 716,\n",
       " 'india': 717,\n",
       " 'kind': 718,\n",
       " 'turkey': 719,\n",
       " 'name': 720,\n",
       " 'presidential': 721,\n",
       " 'short': 722,\n",
       " 'government': 723,\n",
       " 'killing': 724,\n",
       " 'drugs': 725,\n",
       " 'sees': 726,\n",
       " 'working': 727,\n",
       " 'point': 728,\n",
       " 'yet': 729,\n",
       " 'ride': 730,\n",
       " 'gone': 731,\n",
       " 'drag': 732,\n",
       " 'charge': 733,\n",
       " 'hours': 734,\n",
       " 'step': 735,\n",
       " 'now’': 736,\n",
       " 'late': 737,\n",
       " 'press': 738,\n",
       " 'fired': 739,\n",
       " 'die': 740,\n",
       " 'porn': 741,\n",
       " 'liberal': 742,\n",
       " 'sick': 743,\n",
       " 'chaos': 744,\n",
       " 'return': 745,\n",
       " 'watch': 746,\n",
       " 'study': 747,\n",
       " 'system': 748,\n",
       " 'ends': 749,\n",
       " 'memo': 750,\n",
       " 'millions': 751,\n",
       " 'investors': 752,\n",
       " 'add': 753,\n",
       " 'everyone': 754,\n",
       " 'peace': 755,\n",
       " 'turning': 756,\n",
       " 'classic': 757,\n",
       " 'teach': 758,\n",
       " 'let’s': 759,\n",
       " 'fox': 760,\n",
       " 'kushner': 761,\n",
       " 'thinks': 762,\n",
       " 'shot': 763,\n",
       " 'soul': 764,\n",
       " 'same': 765,\n",
       " 'promise': 766,\n",
       " 'guilty': 767,\n",
       " 'moves': 768,\n",
       " 'doctors': 769,\n",
       " 'course': 770,\n",
       " 'la': 771,\n",
       " 'chance': 772,\n",
       " 'shift': 773,\n",
       " 'transgender': 774,\n",
       " 'trouble': 775,\n",
       " 'iran': 776,\n",
       " 'special': 777,\n",
       " 'silicon': 778,\n",
       " 'maybe': 779,\n",
       " 'read': 780,\n",
       " 'magic': 781,\n",
       " 'hands': 782,\n",
       " 'pet': 783,\n",
       " 'kill': 784,\n",
       " 'details': 785,\n",
       " 'cost': 786,\n",
       " 'sleep': 787,\n",
       " 'paris': 788,\n",
       " 'trying': 789,\n",
       " 'effect': 790,\n",
       " 'believe': 791,\n",
       " 'jersey': 792,\n",
       " '‘rupaul’s': 793,\n",
       " 'colbert': 794,\n",
       " 'billion': 795,\n",
       " 'gave': 796,\n",
       " 'housing': 797,\n",
       " 'strikes': 798,\n",
       " 'lot': 799,\n",
       " 'taste': 800,\n",
       " 'noah': 801,\n",
       " 'disaster': 802,\n",
       " 'online': 803,\n",
       " 'very': 804,\n",
       " 'gives': 805,\n",
       " 'eye': 806,\n",
       " 'streets': 807,\n",
       " 'super': 808,\n",
       " 'enough': 809,\n",
       " 'storm': 810,\n",
       " 'gold': 811,\n",
       " 'phone': 812,\n",
       " 'deadly': 813,\n",
       " 'there’s': 814,\n",
       " 'always': 815,\n",
       " 'charges': 816,\n",
       " 'hits': 817,\n",
       " '13': 818,\n",
       " 'hall': 819,\n",
       " 'thing': 820,\n",
       " 'alienist’': 821,\n",
       " 'blue': 822,\n",
       " 'divide': 823,\n",
       " 'tune': 824,\n",
       " '‘homeland': 825,\n",
       " 'clues': 826,\n",
       " 'clash': 827,\n",
       " 'artists': 828,\n",
       " 'box': 829,\n",
       " 'person': 830,\n",
       " 'civil': 831,\n",
       " 'focus': 832,\n",
       " 'used': 833,\n",
       " 'angry': 834,\n",
       " 'equal': 835,\n",
       " 'intelligence': 836,\n",
       " 'amazon': 837,\n",
       " 'industry': 838,\n",
       " 'votes': 839,\n",
       " 'mexico': 840,\n",
       " 'math': 841,\n",
       " 'split': 842,\n",
       " 'clinton': 843,\n",
       " 'nothing': 844,\n",
       " 'deals': 845,\n",
       " 'amid': 846,\n",
       " 'led': 847,\n",
       " 'harassment': 848,\n",
       " 'patients': 849,\n",
       " 'diabetes': 850,\n",
       " 'federal': 851,\n",
       " 'returns': 852,\n",
       " 'voting': 853,\n",
       " 'dark': 854,\n",
       " 'which': 855,\n",
       " 'gap': 856,\n",
       " 'teams': 857,\n",
       " 'hold': 858,\n",
       " 'macron': 859,\n",
       " 'bright': 860,\n",
       " 'driving': 861,\n",
       " 'hero': 862,\n",
       " 'reading': 863,\n",
       " 'books': 864,\n",
       " 'mayor': 865,\n",
       " 'director': 866,\n",
       " 'foreign': 867,\n",
       " 'steps': 868,\n",
       " 'they’re': 869,\n",
       " 'legacy': 870,\n",
       " 'robots': 871,\n",
       " 'rate': 872,\n",
       " 'lower': 873,\n",
       " 'knows': 874,\n",
       " 'technology': 875,\n",
       " 'marijuana': 876,\n",
       " 'medicine': 877,\n",
       " 'avoid': 878,\n",
       " 'uber': 879,\n",
       " 'stars': 880,\n",
       " 'order': 881,\n",
       " 'op': 882,\n",
       " 'land': 883,\n",
       " 'text': 884,\n",
       " 'front': 885,\n",
       " 'google': 886,\n",
       " 'crime': 887,\n",
       " 'quiz': 888,\n",
       " 'heck': 889,\n",
       " 'wonkish': 890,\n",
       " 'finally': 891,\n",
       " 'calling': 892,\n",
       " 'took': 893,\n",
       " 'homeless': 894,\n",
       " 'trevor': 895,\n",
       " 'cash': 896,\n",
       " 'kitchen': 897,\n",
       " 'exercise': 898,\n",
       " 'toll': 899,\n",
       " 'wait': 900,\n",
       " '‘how': 901,\n",
       " 'calls': 902,\n",
       " 'we’re': 903,\n",
       " 'trail': 904,\n",
       " '‘here': 905,\n",
       " 'god': 906,\n",
       " 'stress': 907,\n",
       " 'assassination': 908,\n",
       " 'secrets': 909,\n",
       " 'protests': 910,\n",
       " 'lies': 911,\n",
       " 'abroad': 912,\n",
       " 'photos': 913,\n",
       " 'refugee': 914,\n",
       " 'pass': 915,\n",
       " 'jan': 916,\n",
       " 'sound': 917,\n",
       " 'you’ve': 918,\n",
       " 'cover': 919,\n",
       " 'chef': 920,\n",
       " '11': 921,\n",
       " 'april': 922,\n",
       " 'cause': 923,\n",
       " 'restaurant': 924,\n",
       " 'forward': 925,\n",
       " 'opioids': 926,\n",
       " 'came': 927,\n",
       " 'station': 928,\n",
       " 'adviser': 929,\n",
       " 'seeking': 930,\n",
       " 'park': 931,\n",
       " '50': 932,\n",
       " 'army': 933,\n",
       " 'toward': 934,\n",
       " 'rule': 935,\n",
       " 'stories': 936,\n",
       " 'facing': 937,\n",
       " 'leaves': 938,\n",
       " 'message': 939,\n",
       " 'worse': 940,\n",
       " 'strike': 941,\n",
       " 'language': 942,\n",
       " 'those': 943,\n",
       " 'southern': 944,\n",
       " 'approach': 945,\n",
       " '12': 946,\n",
       " 'false': 947,\n",
       " 'earth': 948,\n",
       " 'families': 949,\n",
       " 'full': 950,\n",
       " 'canada': 951,\n",
       " 'fight’': 952,\n",
       " 'testing': 953,\n",
       " 'flying': 954,\n",
       " 'shifts': 955,\n",
       " 'defying': 956,\n",
       " 'since': 957,\n",
       " 'diet': 958,\n",
       " 'tale’': 959,\n",
       " 'internet': 960,\n",
       " 'murder': 961,\n",
       " 'later': 962,\n",
       " 'deep': 963,\n",
       " 'signs': 964,\n",
       " 'jeff': 965,\n",
       " 'tree': 966,\n",
       " 'reach': 967,\n",
       " 'missile': 968,\n",
       " 'ending': 969,\n",
       " 'cooking': 970,\n",
       " 'running': 971,\n",
       " 'fraud': 972,\n",
       " 'letter': 973,\n",
       " 'lines': 974,\n",
       " 'empire': 975,\n",
       " 'reporting': 976,\n",
       " 'arizona': 977,\n",
       " 'paul': 978,\n",
       " 'movie': 979,\n",
       " 'middle': 980,\n",
       " 'ideas': 981,\n",
       " 'blasio': 982,\n",
       " 'doing': 983,\n",
       " 'pruitt': 984,\n",
       " 'seen': 985,\n",
       " 'marriage': 986,\n",
       " 'trust': 987,\n",
       " 'sanctions': 988,\n",
       " 'flight': 989,\n",
       " 'sea': 990,\n",
       " 'remember': 991,\n",
       " 'ahead': 992,\n",
       " 'issue': 993,\n",
       " 'begins': 994,\n",
       " 'major': 995,\n",
       " 'worst': 996,\n",
       " 'gender': 997,\n",
       " 'vows': 998,\n",
       " 'boss': 999,\n",
       " 'tiny': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating smaller dictionary to visualize tokenization\n",
    "small_dictionary = { key : value for key, value in tokenizer.word_index.items() \\\n",
    "                    if key in ['the', 'plan', 'is', 'to', 'play', 'eat','sleep', 'repeat']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'to': 3, 'is': 9, 'plan': 82, 'eat': 247, 'play': 330, 'sleep': 787, 'repeat': 3226}\n"
     ]
    }
   ],
   "source": [
    "print(small_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [82], [9], [3], [330], [247], [787], [3226]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['the','plan', 'is', 'to', 'play', 'eat','sleep', 'repeat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sequence of tokens for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our model will understand a sequence of tokens(in numbers) instead of the actual word itself\n",
    "\n",
    "all_sequences = []\n",
    "\n",
    "for line in all_headlines:\n",
    "    sequence_of_tokens = tokenizer.texts_to_sequences( [line] )[0] #converting the headline into a sequence of tokens\n",
    "    \n",
    "    for i in range(1, len(sequence_of_tokens)):\n",
    "        partial_sequence = sequence_of_tokens[:i+1]\n",
    "        all_sequences.append(partial_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an', 'finding an expansive', 'finding an expansive view', 'finding an expansive view of', 'finding an expansive view of a']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sequences_to_texts(all_sequences[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[403, 17],\n",
       " [403, 17, 5242],\n",
       " [403, 17, 5242, 543],\n",
       " [403, 17, 5242, 543, 4],\n",
       " [403, 17, 5242, 543, 4, 2]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding our seqeunces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding our sequences to same length in order to train our model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max([len(line) for line in all_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       403,  17])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sequences = np.array(pad_sequences(all_sequences, maxlen = max_seq_len, padding = 'pre'))\n",
    "all_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17, 5242,  543,    4,    2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WE will be using the predictors to predict the target words(labels, the last word in each sequence)\n",
    "\n",
    "# Predictors are every word except the last\n",
    "predictors = all_sequences[:,:-1]\n",
    "# Labels are the last word\n",
    "labels = all_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = utils.to_categorical(labels, num_classes=total_num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our input length is  max_seq_len - 1, as the last word happens to be the label\n",
    "input_len = max_seq_len - 1 \n",
    "\n",
    "model = Sequential()\n",
    "#Adding the input embedding layer with embeddings dim = 10\n",
    "model.add(Embedding(total_num_of_words, 10, input_length=input_len))\n",
    "#Adding a LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "#Adding the output layer\n",
    "model.add(Dense(total_num_of_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 27, 10)            117530    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11753)             1187053   \n",
      "=================================================================\n",
      "Total params: 1,348,983\n",
      "Trainable params: 1,348,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1666/1666 [==============================] - 36s 20ms/step - loss: 7.8913\n",
      "Epoch 2/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 7.4805 0s - l\n",
      "Epoch 3/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 7.2931\n",
      "Epoch 4/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 7.0853\n",
      "Epoch 5/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 6.8548\n",
      "Epoch 6/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 6.6091\n",
      "Epoch 7/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 6.3454\n",
      "Epoch 8/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 6.0892 0s\n",
      "Epoch 9/30\n",
      "1666/1666 [==============================] - 37s 22ms/step - loss: 5.8438\n",
      "Epoch 10/30\n",
      "1666/1666 [==============================] - 43s 26ms/step - loss: 5.6125\n",
      "Epoch 11/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 5.3906\n",
      "Epoch 12/30\n",
      "1666/1666 [==============================] - 39s 23ms/step - loss: 5.1811\n",
      "Epoch 13/30\n",
      "1666/1666 [==============================] - 38s 23ms/step - loss: 4.9882\n",
      "Epoch 14/30\n",
      "1666/1666 [==============================] - 38s 23ms/step - loss: 4.7963\n",
      "Epoch 15/30\n",
      "1666/1666 [==============================] - 41s 24ms/step - loss: 4.6238\n",
      "Epoch 16/30\n",
      "1666/1666 [==============================] - 40s 24ms/step - loss: 4.4580\n",
      "Epoch 17/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 4.3003\n",
      "Epoch 18/30\n",
      "1666/1666 [==============================] - 40s 24ms/step - loss: 4.1529 0s - loss - ETA: 0s - loss: 4.\n",
      "Epoch 19/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.0160 0s - loss: 4.0\n",
      "Epoch 20/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 3.8889\n",
      "Epoch 21/30\n",
      "1666/1666 [==============================] - 36s 22ms/step - loss: 3.7677\n",
      "Epoch 22/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 3.6526\n",
      "Epoch 23/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 3.5426\n",
      "Epoch 24/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 3.4420\n",
      "Epoch 25/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 3.3516\n",
      "Epoch 26/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 3.2647\n",
      "Epoch 27/30\n",
      "1666/1666 [==============================] - 35s 21ms/step - loss: 3.1783\n",
      "Epoch 28/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 3.0950\n",
      "Epoch 29/30\n",
      "1666/1666 [==============================] - 32s 19ms/step - loss: 3.0215\n",
      "Epoch 30/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 2.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1df85f707b8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(texts):\n",
    "    token_sequence = tokenizer.texts_to_sequences([texts])[0]\n",
    "    token_sequence = pad_sequences([token_sequence], maxlen = max_seq_len -1, padding = 'pre')\n",
    "    prediction = model.predict_classes(token_sequence, verbose=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smgee\\Documents\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1442], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next_token = predict_next_token(\"the fear of school\")\n",
    "next_token = predict_next_token(\"today in new york\")\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adds']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word = tokenizer.sequences_to_texts([next_token])\n",
    "next_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Headines!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the previous function and apply it to predict more than one word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines_generator(sample_texts, next_words = 1):\n",
    "    for i in range(next_words):\n",
    "        next_token = predict_next_token(sample_texts)\n",
    "        next_word = tokenizer.sequences_to_texts([next_token])[0]\n",
    "        sample_texts += \" \"+ next_word\n",
    "    return sample_texts.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = ['the fear of school', 'yesterday was a', 'washington dc is', 'today in new york', 'the school district has', 'crime has become', 'in recent news', 'trump has demanded', 'violence is not', 'sports and education must', 'music has become']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fear Of School Honesty Gets Demolition Seems To The\n",
      "Yesterday Was A ‘Regret Clause’ A Story And The\n",
      "Washington Dc Is The New York Hotel A Last\n",
      "Today In New York Adds Trump Stamps And A Lot\n",
      "The School District Has The Grid And Luxurious Spending District\n",
      "Crime Has Become A Pawn To A Lift Israeli\n",
      "In Recent News And Memories To Cover Homeless To\n",
      "Trump Has Demanded Access Of ‘Roseanne’ And Answers To\n",
      "Violence Is Not To Confront A Appetite Of Populism\n",
      "Sports And Education Must Don’T Don’T Don’T Be Easy Books\n",
      "Music Has Become A Cliff War Of A Changing\n"
     ]
    }
   ],
   "source": [
    "for texts in sample_texts:\n",
    "    print(headlines_generator(texts, next_words = 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
